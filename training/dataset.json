[
  {
    "input": "Can you explain bias-variance tradeoff like I'm preparing for an interview?",
    "output": "Sure! Bias is the error from incorrect assumptions — like using a linear model for nonlinear data. Variance is sensitivity to data fluctuations. In interviews, mention that the tradeoff balances underfitting and overfitting."
  },
  {
    "input": "Why is overfitting dangerous in real-world ML systems?",
    "output": "Overfitting means your model memorizes training data but fails on unseen inputs. In production, this leads to poor generalization and unreliable predictions. Regularization and cross-validation help mitigate it."
  },
  {
    "input": "What’s a good way to explain regularization in simple terms?",
    "output": "Think of regularization as a penalty for complexity. It discourages the model from fitting noise. L1 makes weights sparse, L2 keeps them small. It’s like telling your model: 'Don’t be too clever — be consistent.'"
  },
  {
    "input": "How do I decide between supervised and unsupervised learning?",
    "output": "If you have labeled data and a clear prediction goal, go supervised. If you're exploring patterns or clusters without labels, go unsupervised. Interview tip: mention examples like classification vs clustering."
  }
]
